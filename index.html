<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Final Update</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>The Computer Visionaries: Midterm Update</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Rishab Kaup, Karthik Praturu, Noah Sutter and Austen Schunk</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Computer Vision: Final Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Midterm Update -->
<h2><u>Abstract</u></h2>
<!--One or two sentences on the motivation behind the problem you are
solving. One or two sentences describing the approach you took.
One or two sentences on the main result you obtained.-->

Style transfer is a method of transferring the style of an image, like a painting,
to another image. This is useful for creating new types of computer generated
artwork, and it can even be used for videos. Our approach focuses on exploring
and optimizing the actual style transfer for videos, as well as the training of models
used for style transfer. We have found key areas
for improvement in the most popular style transfer algorithm currently in use
(shown in [1]) through our experiments and propose a modified structure for possible better performance, 
and faster training.

<br>
<br>

<div style = "text-align: center">
  <img style = "width: 46%; height:100%" src= "images/style_transfer_ex2.jpg">
  <img style = "width: 47%; height:100%" src= "images/style_transfer_ex1.gif">
</div>

<br>
<br>
<h2><u>Introduction</u></h2>
<!-- Motivation behind the problem you are solving, what applications it has,
any brief background on the particular domain you are working in
(if not regular RBG photographs), etc. If you are using a new way to solve an
existing problem, briefly mention and describe the existing approaches and tell
us how your approach is new. -->

Style transfer algorithms produce very artistic and often seemingly imaginative
outputs, making it a very popular way to filter images and generate abstract
art. Tools like the <a href = "https://deepdreamgenerator.com/" target="_blank">Deep Dream Generator</a>
use style transfer algorithms to generate visual content for consumer use or
research purposes. Real-time style transfer can be used for augmented reality
applications and videos by applying the transformation frame-by-frame, but this
requires faster methods for determining how to transfer style.
<br><br>
The modern style transfer algorithm, introduced by Johnson et al., works as
shown in the following figure:

<br>

<div style = "text-align: center">
  <img style = "width: 46%; height:100%" src= "images/johnsonSTNetwork.png">
</div>
<h5 style = "text-align: center">Johnson et al. Style Tranfer Pipeline [1]</h5>

<br>

The algorithm is split up into two parts, an image transformation network (ITN) and a
loss network. The loss network is a pre-trained VGG16 network meant for
image classification, and it defines weighted functions of feature reconstruction
loss and style reconstruction loss. Most modern style transfer implementations
make use of the same loss network. The loss functions generated by the loss network
are used when training the image transformation network (ITN), and training the
ITN introduced in [1] involves fixing a style image and feeding through a large
database of content images. The ITN introduced by Johnson et al. incorporates a
convolutional neural network with multiple convolutional and pooling layers
throughout; however, other modern implementations use different approaches.
For example, the method introduced in [7] has an ITN that matches the mean
and variance of intermediate features, and the ITN in [8] uses a network that
attempts to minimize the difference of centered covariance between the loss
network output and the combined style and content images.
<br><br>
We approached the problem of style transfer by first taking into account what
currently exists, and then modifying and combining the traits present in each
method to try and achieve an algorithm better suited for modern, high-quality and
fast videos. Specifically, we attempted to make the videos created smoother and 
more pleasing to the eye. Additionally, we wanted to make the styling of images more
accessible. The largest barrier to this is the sheer time taken for training style models
as in [1]. We attempted to find ways to speed up this process as well.
<br><br>

<h2><u>Approach</u></h2>
<!--  Describe very clearly and systematically your approach to solve the problem.
Tell us exactly what existing implementations you used to build your system.
Tell us what obstacles you faced and how you addressed them. Justify any design
 choices or judgment calls you made in your approach.-->

  <h4>Part 1: Smoother Videos</h4>

  In order to create smoother videos, we decided to approach the problem using post processing rather than training. This is because the other major problem that we 
  are attempting to solve is the long training times we have encountered throughout the process. In this vein, there are several method through which we attempted to 
  improve the flickering effect of stylizing videos:
  <br><br>
  <ul>
    <li>Gaussian Filtering</li>
    <li>Interpolation of video (predicting frames between frames)</li>
    <li>Frame subtraction</li> 
  </ul>

  These attempted changes were what we thought had the best chance of success of making the videos smoother, and reduce the flickering effect.
  <br><br>
  <h4>Part 2: Training Time Improvements</h4>

  TODO: AUSTEN PLEASE WRITE ABOUT WHAT YOUR APPROACH WAS


<br><br>
<!-- Experiments and Results -->
<h2><u>Experiments and Results</u></h3>
  <br><br>

  <h4>Part 1: Smoother Videos</h4>
  <h5>Gaussian Filtering</h5>
  <h5>Interpolation of video (predicting frames between frames)</h5>
  <h5>Frame subtraction</h5>

  <div style = "text-align: center">
      <figure>
        <video width="320" height="320" autoplay loop>
            <source src="images/fox_control_udnie.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption>Original</figcaption>
      <figure>
    </div>

  <div style = "text-align: center">
      <figure>
          <video width="320" height="320" autoplay loop>
              <source src="images/fox_use_prev_udnie.mp4" type= "video/mp4">
            Your browser does not support the video tag.
          </video>
          <video width="320" height="320" autoplay loop>
              <source src="images/fox_use_prev_udnie2.mp4" type= "video/mp4">
            Your browser does not support the video tag.
          </video>
          <figcaption>Left: Using previous stylized frame without overlay | Right: Using previous stylized frame with overlay</figcaption>
      </figure>

    </div>
  

<h2><u>Future Work</u></h2>
<br><br>
<h2><u>Conclusion</u></h2>
<br><br>


<br><br><br>

<hr>

Relevant papers and articles:
<br>
<ol>
  <li>Johnson et al. <a href = "https://arxiv.org/pdf/1603.08155.pdf" target="_blank">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li>
  <li>Ruder et al. <a href = "https://arxiv.org/pdf/1604.08610.pdf" target="_blank">Artistic Style Transfer for Video</a></li>
  <li>Cheng et al. <a href="https://arxiv.org/pdf/1710.09282.pdf">A Survey of Model Compression and Acceleration for Deep Neural Networks</a></li>
  <li>Huang et al. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Real-Time_Neural_Style_CVPR_2017_paper.pdf">
    Real-Time Neural Style Transfer for Videos
  </a></li>
  <li>Gao et al. <a href="https://arxiv.org/pdf/1807.01197.pdf">ReCoNet: Real-time Coherent Video StyleTransfer Network</a></li>
  <li>Weinzaepfel et al. <a href="https://hal.inria.fr/hal-00873592/document">DeepFlow: Large displacement optical flow with deep matching</a></li>
  <li>Huang et al. <a href="https://arxiv.org/pdf/1703.06868.pdf" >Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></li>
  <li>Li et al. <a href="https://arxiv.org/pdf/1808.04537v1.pdf">Learning Linear Transformations for Fast Arbitrary Style Transfer</a></li>
</ol>
<br>


<!-- When \(a \ne 0\), there are two solutions to \(ax^2 + bx + c = 0\) and they are
$$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$
<br>
Experiments and results: Describe the experimental setup you will follow, which datasets you will use, which existing code you will exploit, what you will implement yourself, and what you would define as a success for the project. If you plan on collecting your own data, describe what data collection protocol you will follow. Provide a list of experiments you will perform. Describe what you expect the experiments to reveal, or what is uncertain about the potential outcomes.
 -->
  <hr>
  <footer>
  <p>Â© Rishab Kaup, Karthik Praturu, Noah Sutter and Austen Schunk</p>
  </footer>
</div>
</div>

</body></html>
